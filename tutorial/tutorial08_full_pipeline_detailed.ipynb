{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline (detailed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the full pipeline in a detailed manner, including the preprocessing steps, the summerization steps and the classification ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset under the Pandas Dataframe format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Melusine operates Pandas Dataframes by applying functions to certain columns to produce new columns, the initial columns have to follow a strict naming.\n",
    "\n",
    "The basic requirement to use Melusine is to have an input e-mail DataFrame with the following columns :\n",
    "- body : Body of an email (single message or conversation historic)\n",
    "- header : Header of an email\n",
    "- date : Reception date of an email\n",
    "- from : Email address of the sender\n",
    "- to (optional): Email address of the recipient\n",
    "- label (optional): Label of the email for a classification task (examples: Business, Spam, Finance or Family)\n",
    "\n",
    "Each row correspond to a unique email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.408281Z",
     "start_time": "2019-12-17T09:41:53.883080Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.data.data_loader import load_email_data\n",
    "\n",
    "df_emails = load_email_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.434223Z",
     "start_time": "2019-12-17T09:41:54.412573Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.462713Z",
     "start_time": "2019-12-17T09:41:54.437143Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Body :')\n",
    "print(df_emails.body[5])\n",
    "print('\\n')\n",
    "print('Header :')\n",
    "print(df_emails.header[5])\n",
    "print('Date :')\n",
    "print(df_emails.date[5])\n",
    "print('From :')\n",
    "print(df_emails.loc[5,\"from\"])\n",
    "print('To :')\n",
    "print(df_emails.to[5])\n",
    "print('Label :')\n",
    "print(df_emails.label[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to manage transfers and replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single email can contain several replies or transfers in its body.\n",
    "\n",
    "In this pipeline the functions applied are :\n",
    "- **check_mail_begin_by_transfer :** returns True if an email is a direct transfer, else False.\n",
    "- **update_info_for_transfer_mail :** update the columns body, header, date, from and to if the email is a direct transfer.\n",
    "- **add_boolean_answer :** returns True if an email is an answer, else False.\n",
    "- **add_boolean_transfer :** returns True if an email is transferred, else False.\n",
    "\n",
    "This pipeline will create the following new columns :\n",
    "- **is_begin_by_transfer (boolean) :** indicates if the email is a direct transfer, meaning the person whe tranfered a previous email has not written anything on his own. If it is the case, the body, header, date, from and to columns will be updated with the information of the transfered email.\n",
    "- **is_answer (boolean) :** indicates if the body contains replies from previous emails.\n",
    "- **is_transfer (boolean) :** indicates if the body contains transfered emails (not necesseraly a direct transfer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of a direct tranfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.481838Z",
     "start_time": "2019-12-17T09:41:54.465012Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.loc[0,'header'])\n",
    "print(df_emails.loc[0,'date'])\n",
    "print(df_emails.loc[0,'from'])\n",
    "print(df_emails.loc[0,'to'])\n",
    "print(df_emails.loc[0,'body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.792213Z",
     "start_time": "2019-12-17T09:41:54.486313Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.utils.transformer_scheduler import TransformerScheduler\n",
    "\n",
    "from melusine.prepare_email.manage_transfer_reply import check_mail_begin_by_transfer\n",
    "from melusine.prepare_email.manage_transfer_reply import update_info_for_transfer_mail\n",
    "from melusine.prepare_email.manage_transfer_reply import add_boolean_transfer\n",
    "from melusine.prepare_email.manage_transfer_reply import add_boolean_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.812223Z",
     "start_time": "2019-12-17T09:41:54.794291Z"
    }
   },
   "outputs": [],
   "source": [
    "ManageTransferReplyTransformer = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (check_mail_begin_by_transfer, None, ['is_begin_by_transfer']),\n",
    "        (update_info_for_transfer_mail, None, None),\n",
    "        (add_boolean_answer, None, ['is_answer']),\n",
    "        (add_boolean_transfer, None, ['is_transfer'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.851049Z",
     "start_time": "2019-12-17T09:41:54.814194Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails = ManageTransferReplyTransformer.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.869943Z",
     "start_time": "2019-12-17T09:41:54.853054Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An emails previously transfered directly after it has been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.903694Z",
     "start_time": "2019-12-17T09:41:54.875162Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.loc[0,'is_begin_by_transfer'])\n",
    "print(df_emails.loc[0,'header'])\n",
    "print(df_emails.loc[0,'date'])\n",
    "print(df_emails.loc[0,'from'])\n",
    "print(df_emails.loc[0,'to'])\n",
    "print(df_emails.loc[0,'body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Headers of emails containing replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.932236Z",
     "start_time": "2019-12-17T09:41:54.909109Z"
    }
   },
   "outputs": [],
   "source": [
    "test = df_emails[df_emails['is_answer']==True]\n",
    "test.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Headers of emails containing transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:54.956675Z",
     "start_time": "2019-12-17T09:41:54.935461Z"
    }
   },
   "outputs": [],
   "source": [
    "test = df_emails[df_emails['is_transfer']==True]\n",
    "test.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email segmenting pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each email will be segmented according to :\n",
    "- the different messages\n",
    "- the metadata, the header and the  text of each messages\n",
    "- the type of metadata (date, from, to)\n",
    "- the different partos of each text (hello, greetings, footer..)\n",
    "\n",
    "In this pipeline the functions applied are :\n",
    "- **build_historic :** segments the different messages of the body and returns a list of dictionaries, one per message. Each dictionary has a key 'meta' to access the metadata and a key 'text' to access the text of the body.\n",
    "- **structure_email :** splits parts of each messages in historic, tags them (tags: Hello, Body, Greetings, etc) and segments each part of the metadata (date, from, to). The result is returned as a list of dictionaries, one per message. Each dictionary has a key 'meta' to access the metadata (itself a dictionary with keys 'date', 'from' and 'to') and a key 'text' to access the text of the body (itself a dictionary with keys 'header' and 'structured_text').\n",
    "\n",
    "This pipeline creates the following new columns :\n",
    "- **structured_historic :** the list of dictionaries returned by **build_historic** function.\n",
    "- **structured_body :** the list of dictionaries returned by **structure_email** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.117210Z",
     "start_time": "2019-12-17T09:41:54.959041Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.prepare_email.build_historic import build_historic\n",
    "from melusine.prepare_email.mail_segmenting import structure_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.136505Z",
     "start_time": "2019-12-17T09:41:55.118954Z"
    }
   },
   "outputs": [],
   "source": [
    "SegmentingTransformer = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (build_historic, None, ['structured_historic']),\n",
    "        (structure_email, None, ['structured_body'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.268524Z",
     "start_time": "2019-12-17T09:41:55.138000Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails = SegmentingTransformer.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.289349Z",
     "start_time": "2019-12-17T09:41:55.270513Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.316153Z",
     "start_time": "2019-12-17T09:41:55.291891Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.body[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.339306Z",
     "start_time": "2019-12-17T09:41:55.319242Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.structured_historic[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.362569Z",
     "start_time": "2019-12-17T09:41:55.340970Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.structured_body[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction and cleaning of the body of the last message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once each email segmented, the body of the last message will be extracted and cleaned.\n",
    "\n",
    "In this pipeline the functions applied are :\n",
    "- **extract_last_body :** returns the body of the last message of the email.\n",
    "- **clean_body :** returns the body of the last message of the email after cleaning.\n",
    "\n",
    "This pipeline returns the following columns : \n",
    "- **last_body :** the body of the last message of the email returned by **extract_last_body** function.\n",
    "- **clean_body :** the cleaned body of the last message of the email returned by **clean_body** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.382932Z",
     "start_time": "2019-12-17T09:41:55.364719Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.prepare_email.body_header_extraction import extract_last_body\n",
    "from melusine.prepare_email.cleaning import clean_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.402654Z",
     "start_time": "2019-12-17T09:41:55.386126Z"
    }
   },
   "outputs": [],
   "source": [
    "LastBodyHeaderCleaningTransformer = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (extract_last_body, None, ['last_body']),\n",
    "        (clean_body, None, ['clean_body'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.434264Z",
     "start_time": "2019-12-17T09:41:55.405203Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails = LastBodyHeaderCleaningTransformer.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.451596Z",
     "start_time": "2019-12-17T09:41:55.435517Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.471445Z",
     "start_time": "2019-12-17T09:41:55.453544Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.body[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.491551Z",
     "start_time": "2019-12-17T09:41:55.473608Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.last_body[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:55.511284Z",
     "start_time": "2019-12-17T09:41:55.493368Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.clean_body[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A phraser can be passed on the body. However it first need to be trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.001930Z",
     "start_time": "2019-12-17T09:41:55.513942Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.phraser import Phraser\n",
    "from melusine.nlp_tools.phraser import phraser_on_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.072445Z",
     "start_time": "2019-12-17T09:41:56.003842Z"
    }
   },
   "outputs": [],
   "source": [
    "phraser = Phraser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.125737Z",
     "start_time": "2019-12-17T09:41:56.074432Z"
    }
   },
   "outputs": [],
   "source": [
    "phraser.train(df_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **phraser_on_body** function applies a phraser on the clean_body of an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.147321Z",
     "start_time": "2019-12-17T09:41:56.128200Z"
    }
   },
   "outputs": [],
   "source": [
    "PhraserTransformer = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (phraser_on_body, (phraser,), ['clean_body'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.180963Z",
     "start_time": "2019-12-17T09:41:56.149500Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails = PhraserTransformer.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.202046Z",
     "start_time": "2019-12-17T09:41:56.182420Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.225853Z",
     "start_time": "2019-12-17T09:41:56.204330Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_column=\"clean_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.256166Z",
     "start_time": "2019-12-17T09:41:56.228322Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails = tokenizer.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.279132Z",
     "start_time": "2019-12-17T09:41:56.258175Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.305575Z",
     "start_time": "2019-12-17T09:41:56.281622Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.clean_body[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.328017Z",
     "start_time": "2019-12-17T09:41:56.307957Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_emails.tokens[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata have to be extracted before being dummified.\n",
    "\n",
    "This pipeline extractes the following metadata :\n",
    "- **extension :** from the \"from\" column.\n",
    "- **dayofweek :** from the date.\n",
    "- **hour :** from the date.\n",
    "- **min :** from the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.430369Z",
     "start_time": "2019-12-17T09:41:56.330334Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from melusine.prepare_email.metadata_engineering import MetaExtension\n",
    "from melusine.prepare_email.metadata_engineering import MetaDate\n",
    "from melusine.prepare_email.metadata_engineering import Dummifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.452324Z",
     "start_time": "2019-12-17T09:41:56.432124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline to extract dummified metadata\n",
    "MetadataPipeline = Pipeline([\n",
    "    ('MetaExtension', MetaExtension()),\n",
    "    ('MetaDate', MetaDate()),\n",
    "    ('Dummifier', Dummifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.504109Z",
     "start_time": "2019-12-17T09:41:56.454148Z"
    }
   },
   "outputs": [],
   "source": [
    "df_meta = MetadataPipeline.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.528115Z",
     "start_time": "2019-12-17T09:41:56.506250Z"
    }
   },
   "outputs": [],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.565604Z",
     "start_time": "2019-12-17T09:41:56.530473Z"
    }
   },
   "outputs": [],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a tokens column exists, keywords can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.637969Z",
     "start_time": "2019-12-17T09:41:56.568696Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.summarizer.keywords_generator import KeywordsGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:41:56.701892Z",
     "start_time": "2019-12-17T09:41:56.667861Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_generator = KeywordsGenerator(n_max_keywords=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.519690Z",
     "start_time": "2019-12-17T09:41:56.713551Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails = keywords_generator.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.567401Z",
     "start_time": "2019-12-17T09:42:07.527003Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.clean_body[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.605300Z",
     "start_time": "2019-12-17T09:42:07.570400Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.tokens[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.640063Z",
     "start_time": "2019-12-17T09:42:07.609045Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails.keywords[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melusine offers a NeuralModel class to train, save, load and use for prediction any kind of neural networks based on Keras. \n",
    "Predefined architectures of RNN and CNN models using the cleaned body and the metadata of the emails are also offered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings have to be pretrained on the data set to be given as arguments of the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.739655Z",
     "start_time": "2019-12-17T09:42:07.644713Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.820989Z",
     "start_time": "2019-12-17T09:42:07.742860Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embedding = Embedding(input_column='clean_body',\n",
    "                                 workers=1,\n",
    "                                 min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.873784Z",
     "start_time": "2019-12-17T09:42:07.822775Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embedding.train(df_emails) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation de X et de y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.899835Z",
     "start_time": "2019-12-17T09:42:07.878259Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.925687Z",
     "start_time": "2019-12-17T09:42:07.902050Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.concat([df_emails['clean_body'],df_meta],axis=1)\n",
    "y = df_emails['label']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.953913Z",
     "start_time": "2019-12-17T09:42:07.927748Z"
    }
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:07.991202Z",
     "start_time": "2019-12-17T09:42:07.956613Z"
    }
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:08.015958Z",
     "start_time": "2019-12-17T09:42:07.993878Z"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement et prédictions avec un CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:09.195894Z",
     "start_time": "2019-12-17T09:42:08.017738Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.models.neural_architectures import cnn_model\n",
    "from melusine.models.train import NeuralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:09.245860Z",
     "start_time": "2019-12-17T09:42:09.198093Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_model = NeuralModel(architecture_function=cnn_model,\n",
    "                       pretrained_embedding=pretrained_embedding,\n",
    "                       text_input_column=\"clean_body\",\n",
    "                       meta_input_list=['extension', 'dayofweek','hour', 'min'],\n",
    "                       n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:13.200236Z",
     "start_time": "2019-12-17T09:42:09.249731Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:13.439946Z",
     "start_time": "2019-12-17T09:42:13.202560Z"
    }
   },
   "outputs": [],
   "source": [
    "y_res = nn_model.predict(X)\n",
    "y_res = le.inverse_transform(y_res)\n",
    "y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a dict instead of a Dataframe as input (performance optimization)\n",
    "\n",
    "In an industrialized context, a trained model might be fed input data one by one.  \n",
    "In this case, creating a pandas DataFrame with a single row is overkill and massive performed gain can be obtained by using a dict instead of a DataFrame.  \n",
    "\n",
    "\n",
    "Melusine is developped to ensure dict compatibility as described in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:42:25.562424Z",
     "start_time": "2019-12-17T09:42:25.466318Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# ============== Test dict compatibility ==============\n",
    "dict_emails = df_emails.to_dict(orient='records')[0]\n",
    "dict_meta = MetadataPipeline.transform(dict_emails)\n",
    "dict_keywords = keywords_generator.transform(dict_emails)\n",
    "\n",
    "dict_input = copy.deepcopy(dict_meta)\n",
    "dict_input['clean_body'] = dict_emails['clean_body']\n",
    "\n",
    "dict_result = nn_model.predict(dict_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
