{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tools tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nlp_tools** subpackage offers classic NLP tools implemented as classes that will be used to preprocess an already cleaned text :\n",
    "- a **Phraser class** : to transform common multi-word expressions into single elements (*new york* becomes *new_york*)\n",
    "- a **Tokenizer class** : to split a sentence-like string into a list of sub-strings (tokens).\n",
    "- an **Embedding class** : to represent of words in a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Phraser class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Phraser class transforms common multi-word expressions into single elements: for example *new york* becomes *new_york*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Phraser object are:\n",
    "- **input_column :** the name of the column of the dataframe that will be used as input for the training of the Phraser.\n",
    "- **common_terms :** list of stopwords to be ignored. The default list is defined in the *conf.json* file.\n",
    "- **threshold :** threshold to select collocations.\n",
    "- **min_count :** minimum count of word to be selected as collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.104064Z",
     "start_time": "2019-09-03T13:16:54.128719Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.phraser import Phraser\n",
    "\n",
    "phraser = Phraser(input_column='clean_body',\n",
    "                  threshold=10,\n",
    "                  min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataframe must contain a column with a clean text : **a sentence-like string with only lowcase letters and no accents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.120731Z",
     "start_time": "2019-09-03T13:16:56.108665Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_emails_clean = pd.read_csv('./data/emails_preprocessed.csv', encoding='utf-8', sep=';')\n",
    "df_emails_clean = df_emails_clean[['clean_body']]\n",
    "df_emails_clean = df_emails_clean.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.148090Z",
     "start_time": "2019-09-03T13:16:56.123682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je vous informe que la nouvelle immatriculation est enfin faite. je vous prie de trouver donc la carte grise ainsi que la nouvelle immatriculation. je vous demanderai de faire les changements necessaires concernant l assurance.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails_clean.clean_body[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.190813Z",
     "start_time": "2019-09-03T13:16:56.150379Z"
    }
   },
   "outputs": [],
   "source": [
    "phraser.train(df_emails_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.205955Z",
     "start_time": "2019-09-03T13:16:56.192989Z"
    }
   },
   "outputs": [],
   "source": [
    "phraser.save('./data/phraser.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.281543Z",
     "start_time": "2019-09-03T13:16:56.207874Z"
    }
   },
   "outputs": [],
   "source": [
    "phraser = Phraser().load('./data/phraser.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method of a Phraser object is its *train* method. To apply a specifi phraser it has to be passed as an argument to one of the following functions :\n",
    "- **phraser_on_body :** to apply the phraser on the *clean_body* column of a dataframe\n",
    "- **phraser_on_header :** to apply the phraser on the *clean_header* column of a dataframe\n",
    "\n",
    "The **phraser_on_body** and **phraser_on_header** functions are applied on rows of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.290187Z",
     "start_time": "2019-09-03T13:16:56.283286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je vous informe que la nouvelle immatriculation est enfin faite. je vous prie de trouver donc la carte grise ainsi que la nouvelle immatriculation. je vous demanderai de faire les changements necessaires concernant l assurance.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from melusine.nlp_tools.phraser import phraser_on_body\n",
    "\n",
    "row = df_emails_clean.loc[1,:]\n",
    "\n",
    "phraser_on_body(row, phraser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the **phraser_on_body** and **phraser_on_header** functions are applied on rows of dataframes, they have to be passed as arguments of a **TransferScheduler object** in order to be applied on a whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.300132Z",
     "start_time": "2019-09-03T13:16:56.292465Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.utils.transformer_scheduler import TransformerScheduler\n",
    "\n",
    "PhraserTransformer = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (phraser_on_body, (phraser,), ['clean_body'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.325460Z",
     "start_time": "2019-09-03T13:16:56.306889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails_clean = PhraserTransformer.fit_transform(df_emails_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer class splits a sentence-like string into a list of sub-strings (tokens). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Tokenizer object are :\n",
    "- **input_column :** the input text column to consider for the tokenizer.\n",
    "- **stopwords :** the list of words to remove from list of tokens. Default value, list defined in conf.json file.\n",
    "- **stop_removal :** True if stopwords to be removed, else False. Default value, False.\n",
    "- **n_jobs :** the number of cores used for computation. Default value, 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.340793Z",
     "start_time": "2019-09-03T13:16:56.335017Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer (input_column='clean_body',\n",
    "                       stop_removal=True,\n",
    "                       n_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **fit_transform** method on a dataframe to create a new ***tokens* column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.369279Z",
     "start_time": "2019-09-03T13:16:56.345282Z"
    }
   },
   "outputs": [],
   "source": [
    "df_emails_clean = tokenizer.fit_transform(df_emails_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.376455Z",
     "start_time": "2019-09-03T13:16:56.371533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je vous informe que la nouvelle immatriculation est enfin faite. je vous prie de trouver donc la carte grise ainsi que la nouvelle immatriculation. je vous demanderai de faire les changements necessaires concernant l assurance.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails_clean.clean_body[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.382607Z",
     "start_time": "2019-09-03T13:16:56.378519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['informe',\n",
       " 'nouvelle',\n",
       " 'immatriculation',\n",
       " 'enfin',\n",
       " 'faite',\n",
       " 'prie',\n",
       " 'trouver',\n",
       " 'donc',\n",
       " 'carte',\n",
       " 'grise',\n",
       " 'ainsi',\n",
       " 'nouvelle',\n",
       " 'immatriculation',\n",
       " 'demanderai',\n",
       " 'faire',\n",
       " 'les',\n",
       " 'changements',\n",
       " 'necessaires',\n",
       " 'concernant',\n",
       " 'assurance']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails_clean.tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.580240Z",
     "start_time": "2019-09-03T13:16:56.384169Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "_ = joblib.dump(tokenizer,\"./data/tokenizer.pickle\",compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.704821Z",
     "start_time": "2019-09-03T13:16:56.582517Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = joblib.load(\"./data/tokenizer.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are abstract representations of words in a lower dimensional vector space. One of the advantages of word embeddings is thus to save computational cost. The Melusine Embedding class uses a **Word2Vec** model. The trained Embedding object will be used in the Models subpackage to train a Neural Network to classify emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of an Embedding object are :\n",
    "- **input_column :** the name of the column used as an input for the training.\n",
    "- **workers :** the number of cores used for computation. Default value, 40.\n",
    "- **seed :** seed for the embedding model,\n",
    "- **iter :** number of iterations for the training,\n",
    "- **size :** dimension of the embeddings\n",
    "- **window :** \n",
    "- **min_count :** minimum number of occurences for a word to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.940504Z",
     "start_time": "2019-09-03T13:16:56.707282Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding\n",
    "\n",
    "embedding = Embedding(input_column='clean_body',\n",
    "                      size=300,\n",
    "                      workers=4,\n",
    "                      min_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.983525Z",
     "start_time": "2019-09-03T13:16:56.942514Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding.train(df_emails_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.989814Z",
     "start_time": "2019-09-03T13:16:56.985064Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding.save('./data/embedding.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:57.053209Z",
     "start_time": "2019-09-03T13:16:56.992001Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = Embedding().load('./data/embedding.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types of embedding available in the **Embedding** class are:\n",
    "- `lsa_docterm` : Apply a Singular Value Decomposition (SVD) on the DocTerm matrix\n",
    "- `lsa_tfidf` : Apply a Singular Value Decomposition (SVD) on the TfIdf matrix\n",
    "- `word2vec_sg` : Train a Word2Vec model using the Skip-Gram method (Warning : time consuming!)\n",
    "- `word2vec_ns` : Train a Word2Vec model using the Negative-Sampling method\n",
    "- `word2vec_cbow` : Train a Word2Vec model using the Continuous Bag-Of-Words method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:57.153164Z",
     "start_time": "2019-09-03T13:16:57.055444Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = Embedding(input_column='clean_body',\n",
    "                      size=300,\n",
    "                      min_count=3,\n",
    "                      method = 'lsa_tfidf'\n",
    "                     )\n",
    "embedding.train(df_emails_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T09:48:50.221008Z",
     "start_time": "2019-09-03T09:48:50.217998Z"
    }
   },
   "source": [
    "### Specify a tokens column instead of a text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to provide the text input to the **Embedding** class:\n",
    "- `input_column` : Provide a raw text column.  The embedding class will tokenize it and create a tokens generator.  The tokens generator will be used to generate tokens as input for training the model\n",
    "- `tokens_column` : Provide a column containing list of tokens.   The embedding class will use a list of list of tokens to train the embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:57.345160Z",
     "start_time": "2019-09-03T13:16:57.154564Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = Embedding(tokens_column='tokens',\n",
    "                      size=300,\n",
    "                      min_count=3,\n",
    "                     )\n",
    "embedding.train(df_emails_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
